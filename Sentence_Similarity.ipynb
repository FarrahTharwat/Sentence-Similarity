{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FarrahTharwat/Sentence-Similarity/blob/main/Sentence_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA0-jdxAkj4y"
      },
      "source": [
        "**Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RoSa8NPkDLy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-H0sEC_ksDb"
      },
      "source": [
        "**Sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOg_LTdWkjN7"
      },
      "outputs": [],
      "source": [
        "# Sample sentences and their similarity scores\n",
        "sentences = [\n",
        "    (\"The sun is shining\", \"The weather is beautiful today\", 0.8),\n",
        "    (\"The sun is shining\", \"It is raining cats and dogs\", 0.2),\n",
        "    (\"The car is red\", \"The apple is red\", 0.7),\n",
        "    (\"This is a sentence\", \"This is a completely different sentence\", 0.3),\n",
        "    (\"He enjoys playing football\", \"Football is his favorite sport\", 0.9),\n",
        "    (\"The book was laid on the table\", \"The table held several books\", 0.7),\n",
        "    (\"She loves to read horror novels\", \"Reading horror stories is her hobby\", 0.9),\n",
        "    (\"Birds fly south in the winter\", \"In winter, birds migrate south\", 0.8),\n",
        "    (\"He is a software engineer\", \"He writes code\", 0.6),\n",
        "    (\"Climate change is a global issue\", \"Global warming affects the earth\", 0.8),\n",
        "    (\"Water boils at 100 degrees Celsius\", \"Boiling point of water is 100Â°C\", 0.9),\n",
        "    (\"She moved to New York last year\", \"Last year, she relocated to New York\", 0.9),\n",
        "    (\"The museum is closed on Mondays\", \"On Mondays, the museum isn't open\", 0.9),\n",
        "    (\"I love eating strawberries\", \"Strawberries are my favorite fruit\", 0.8),\n",
        "    (\"They won their first soccer match\", \"Their team lost the soccer game\", 0.2),\n",
        "    (\"He has two siblings\", \"He is an only child\", 0.1),\n",
        "    (\"The film started at nine o'clock\", \"The movie began at 9 PM\", 0.9),\n",
        "    (\"Our cat is very old\", \"Our pet is quite young\", 0.2),\n",
        "    (\"She's studying to become a lawyer\", \"She is attending law school\", 0.9),\n",
        "    (\"He's allergic to peanuts\", \"Peanut allergies affect him\", 0.9),\n",
        "    (\"They're looking forward to the trip\", \"The upcoming trip excites them\", 0.8),\n",
        "    (\"I need to charge my phone\", \"My phone battery is dead\", 0.6),\n",
        "    (\"Can you call me later?\", \"Please phone me afterwards\", 0.8),\n",
        "    (\"The coffee is too hot to drink\", \"The drink is cold\", 0.2),\n",
        "    (\"Rainforests are located near the equator\", \"Equatorial regions have rainforests\", 0.9),\n",
        "    (\"Mathematics is challenging for many students\", \"Many students struggle with math\", 0.8),\n",
        "    (\"The Earth orbits the Sun\", \"The Sun is orbited by the Earth\", 0.9),\n",
        "    (\"The chef cooked a delicious meal\", \"A tasty dinner was prepared by the chef\", 0.9),\n",
        "    (\"He is reading a novel\", \"He is watching a movie\", 0.2),\n",
        "    (\"My favorite season is spring\", \"I love the springtime\", 0.9),\n",
        "    (\"The dog barked loudly\", \"A loud noise was made by the dog\", 0.8),\n",
        "    (\"The concert starts at eight o'clock\", \"The show begins at 8 PM\", 0.9),\n",
        "    (\"She cut her hair short\", \"She has long hair\", 0.1),\n",
        "    (\"They painted the room blue\", \"The room was painted red\", 0.2),\n",
        "    (\"He runs every morning\", \"Running is his morning routine\", 0.8),\n",
        "    (\"Lightning usually precedes thunder\", \"Thunder follows lightning\", 0.9),\n",
        "    (\"The boy broke the window\", \"The window was broken by the girl\", 0.3),\n",
        "    (\"It rained the whole day\", \"The rain lasted all day\", 0.9),\n",
        "    (\"The cake recipe calls for eggs\", \"Eggs are needed for the cake\", 0.9),\n",
        "    (\"She took a flight to Rome\", \"She drove to Rome\", 0.2),\n",
        "    (\"Whales are mammals\", \"Whales are not fish\", 0.8),\n",
        "    (\"The meal was very satisfying\", \"Dinner was quite disappointing\", 0.2),\n",
        "    (\"I turned off the light\", \"The light was switched off\", 0.9),\n",
        "    (\"He forgot his wallet at home\", \"He left his wallet at home\", 0.9),\n",
        "    (\"The planet Mars is red\", \"Mars is known as the Red Planet\", 0.9),\n",
        "    (\"The building was very tall\", \"It was a short building\", 0.2),\n",
        "    (\"The painting is a masterpiece\", \"The artwork is mediocre\", 0.3),\n",
        "    (\"She adopted a puppy\", \"She adopted a kitten\", 0.3),\n",
        "    (\"The laptop is new\", \"The computer is old\", 0.2),\n",
        "    (\"I learned a lot from the lecture\", \"The lecture was informative\", 0.8),\n",
        "    (\"The phone is ringing\", \"Someone is calling\", 0.7),\n",
        "    (\"The exam was very difficult\", \"The test was easy\", 0.2),\n",
        "    (\"He likes to travel\", \"Traveling is his hobby\", 0.9),\n",
        "    (\"The tree was very old\", \"The ancient tree was tall\", 0.7)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXloKlSkkyuR"
      },
      "source": [
        "**Data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxv3tyFznAjM"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    return sentence.lower().strip()\n",
        "\n",
        "def get_sequences(tokenizer, sentences, max_length):\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    return pad_sequences(sequences, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SraJ5zLLnDuO"
      },
      "outputs": [],
      "source": [
        "# Prepare text data for Siamese Network\n",
        "text_data1 = [preprocess_sentence(pair[0]) for pair in sentences]\n",
        "text_data2 = [preprocess_sentence(pair[1]) for pair in sentences]\n",
        "all_text_data = text_data1 + text_data2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIa2S_iqnFLN"
      },
      "outputs": [],
      "source": [
        "# Tokenization and encoding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_text_data)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_QvhQ2KnGwM"
      },
      "outputs": [],
      "source": [
        "# Set a consistent sequence length\n",
        "max_length = max(max(len(item.split()) for item in all_text_data), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGwZisvvnSsN"
      },
      "outputs": [],
      "source": [
        "sequences1 = get_sequences(tokenizer, text_data1, max_length)\n",
        "sequences2 = get_sequences(tokenizer, text_data2, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mfLaFwRnXGW"
      },
      "outputs": [],
      "source": [
        "labels = np.array([float(score) for _, _, score in sentences])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kSQ9nXrnepi"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfeOXYx1ncCP"
      },
      "outputs": [],
      "source": [
        "def siamese_model(vocab_size, embedding_dim, lstm_units, max_length):\n",
        "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_length)\n",
        "    lstm_layer = LSTM(lstm_units)\n",
        "\n",
        "    input_1 = Input(shape=(max_length,))\n",
        "    input_2 = Input(shape=(max_length,))\n",
        "\n",
        "    encoded_1 = lstm_layer(embedding_layer(input_1))\n",
        "    encoded_2 = lstm_layer(embedding_layer(input_2))\n",
        "\n",
        "    distance = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([encoded_1, encoded_2])\n",
        "    outputs = Dense(1, activation='sigmoid')(distance)\n",
        "\n",
        "    model = Model(inputs=[input_1, input_2], outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzedPQzcnoeP"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt5Az3synpA0"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "embedding_dim = 128\n",
        "lstm_units = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhumDVlUnwgR"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFxNzppDnjwr",
        "outputId": "b92175cd-b473-466e-ffde-bc2f4516f6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/137\n",
            "1/1 [==============================] - 14s 14s/step - loss: 0.6931 - accuracy: 0.0000e+00\n",
            "Epoch 2/137\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.6917 - accuracy: 0.0000e+00\n",
            "Epoch 3/137\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.6904 - accuracy: 0.0000e+00\n",
            "Epoch 4/137\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.6892 - accuracy: 0.0000e+00\n",
            "Epoch 5/137\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.6880 - accuracy: 0.0000e+00\n",
            "Epoch 6/137\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.6867 - accuracy: 0.0000e+00\n",
            "Epoch 7/137\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.6853 - accuracy: 0.0000e+00\n",
            "Epoch 8/137\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.6838 - accuracy: 0.0000e+00\n",
            "Epoch 9/137\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.6822 - accuracy: 0.0000e+00\n",
            "Epoch 10/137\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.6804 - accuracy: 0.0000e+00\n",
            "Epoch 11/137\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.6784 - accuracy: 0.0000e+00\n",
            "Epoch 12/137\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.6761 - accuracy: 0.0000e+00\n",
            "Epoch 13/137\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.6735 - accuracy: 0.0000e+00\n",
            "Epoch 14/137\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.6706 - accuracy: 0.0000e+00\n",
            "Epoch 15/137\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.6674 - accuracy: 0.0000e+00\n",
            "Epoch 16/137\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.6636 - accuracy: 0.0000e+00\n",
            "Epoch 17/137\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.6594 - accuracy: 0.0000e+00\n",
            "Epoch 18/137\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.6545 - accuracy: 0.0000e+00\n",
            "Epoch 19/137\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6490 - accuracy: 0.0000e+00\n",
            "Epoch 20/137\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.6427 - accuracy: 0.0000e+00\n",
            "Epoch 21/137\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.6356 - accuracy: 0.0000e+00\n",
            "Epoch 22/137\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.6275 - accuracy: 0.0000e+00\n",
            "Epoch 23/137\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.6185 - accuracy: 0.0000e+00\n",
            "Epoch 24/137\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.6086 - accuracy: 0.0000e+00\n",
            "Epoch 25/137\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.5979 - accuracy: 0.0000e+00\n",
            "Epoch 26/137\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.5866 - accuracy: 0.0000e+00\n",
            "Epoch 27/137\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.5753 - accuracy: 0.0000e+00\n",
            "Epoch 28/137\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.5644 - accuracy: 0.0000e+00\n",
            "Epoch 29/137\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.5546 - accuracy: 0.0000e+00\n",
            "Epoch 30/137\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.5465 - accuracy: 0.0000e+00\n",
            "Epoch 31/137\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.5405 - accuracy: 0.0000e+00\n",
            "Epoch 32/137\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.5365 - accuracy: 0.0000e+00\n",
            "Epoch 33/137\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.5338 - accuracy: 0.0000e+00\n",
            "Epoch 34/137\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.5311 - accuracy: 0.0000e+00\n",
            "Epoch 35/137\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.5275 - accuracy: 0.0000e+00\n",
            "Epoch 36/137\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.5224 - accuracy: 0.0000e+00\n",
            "Epoch 37/137\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.5164 - accuracy: 0.0000e+00\n",
            "Epoch 38/137\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.5102 - accuracy: 0.0000e+00\n",
            "Epoch 39/137\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5047 - accuracy: 0.0000e+00\n",
            "Epoch 40/137\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.5005 - accuracy: 0.0000e+00\n",
            "Epoch 41/137\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4966 - accuracy: 0.0000e+00\n",
            "Epoch 42/137\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.4923 - accuracy: 0.0000e+00\n",
            "Epoch 43/137\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.4871 - accuracy: 0.0000e+00\n",
            "Epoch 44/137\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.4813 - accuracy: 0.0000e+00\n",
            "Epoch 45/137\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.4757 - accuracy: 0.0000e+00\n",
            "Epoch 46/137\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.4709 - accuracy: 0.0000e+00\n",
            "Epoch 47/137\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.4670 - accuracy: 0.0000e+00\n",
            "Epoch 48/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4642 - accuracy: 0.0000e+00\n",
            "Epoch 49/137\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4622 - accuracy: 0.0000e+00\n",
            "Epoch 50/137\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4610 - accuracy: 0.0000e+00\n",
            "Epoch 51/137\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4602 - accuracy: 0.0000e+00\n",
            "Epoch 52/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4596 - accuracy: 0.0000e+00\n",
            "Epoch 53/137\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4590 - accuracy: 0.0000e+00\n",
            "Epoch 54/137\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4582 - accuracy: 0.0000e+00\n",
            "Epoch 55/137\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4570 - accuracy: 0.0000e+00\n",
            "Epoch 56/137\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4555 - accuracy: 0.0000e+00\n",
            "Epoch 57/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4543 - accuracy: 0.0000e+00\n",
            "Epoch 58/137\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4536 - accuracy: 0.0000e+00\n",
            "Epoch 59/137\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4536 - accuracy: 0.0000e+00\n",
            "Epoch 60/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4534 - accuracy: 0.0000e+00\n",
            "Epoch 61/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4531 - accuracy: 0.0000e+00\n",
            "Epoch 62/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4528 - accuracy: 0.0000e+00\n",
            "Epoch 63/137\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4526 - accuracy: 0.0000e+00\n",
            "Epoch 64/137\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4528 - accuracy: 0.0000e+00\n",
            "Epoch 65/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4531 - accuracy: 0.0000e+00\n",
            "Epoch 66/137\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4532 - accuracy: 0.0000e+00\n",
            "Epoch 67/137\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4530 - accuracy: 0.0000e+00\n",
            "Epoch 68/137\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4527 - accuracy: 0.0000e+00\n",
            "Epoch 69/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4524 - accuracy: 0.0000e+00\n",
            "Epoch 70/137\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4523 - accuracy: 0.0000e+00\n",
            "Epoch 71/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4523 - accuracy: 0.0000e+00\n",
            "Epoch 72/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4523 - accuracy: 0.0000e+00\n",
            "Epoch 73/137\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4522 - accuracy: 0.0000e+00\n",
            "Epoch 74/137\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4521 - accuracy: 0.0000e+00\n",
            "Epoch 75/137\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4521 - accuracy: 0.0000e+00\n",
            "Epoch 76/137\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4520 - accuracy: 0.0000e+00\n",
            "Epoch 77/137\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4520 - accuracy: 0.0000e+00\n",
            "Epoch 78/137\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4520 - accuracy: 0.0000e+00\n",
            "Epoch 79/137\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4520 - accuracy: 0.0000e+00\n",
            "Epoch 80/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4519 - accuracy: 0.0000e+00\n",
            "Epoch 81/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4519 - accuracy: 0.0000e+00\n",
            "Epoch 82/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4519 - accuracy: 0.0000e+00\n",
            "Epoch 83/137\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.4519 - accuracy: 0.0000e+00\n",
            "Epoch 84/137\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4519 - accuracy: 0.0000e+00\n",
            "Epoch 85/137\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 86/137\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 87/137\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 88/137\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 89/137\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 90/137\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 91/137\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 92/137\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 93/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 94/137\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 95/137\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 96/137\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 97/137\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 98/137\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 99/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 100/137\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 101/137\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 102/137\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4518 - accuracy: 0.0000e+00\n",
            "Epoch 103/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 104/137\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 105/137\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 106/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 107/137\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 108/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 109/137\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 110/137\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 111/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 112/137\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 113/137\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 114/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 115/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 116/137\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 117/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 118/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 119/137\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 120/137\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 121/137\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 122/137\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 123/137\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 124/137\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 125/137\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 126/137\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 127/137\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 128/137\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 129/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 130/137\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 131/137\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 132/137\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 133/137\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 134/137\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 135/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 136/137\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n",
            "Epoch 137/137\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4517 - accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fb717719480>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Create and train the model\n",
        "model = siamese_model(vocab_size, embedding_dim, lstm_units, max_length)\n",
        "model.fit([sequences1, sequences2], labels, epochs=137, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict"
      ],
      "metadata": {
        "id": "xVAzVrhHt8GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting similarity for new sentences\n",
        "def predict_similarity(model, sentence1, sentence2, tokenizer, max_length):\n",
        "    preprocessed1 = preprocess_sentence(sentence1)\n",
        "    preprocessed2 = preprocess_sentence(sentence2)\n",
        "    sequence1 = get_sequences(tokenizer, [preprocessed1], max_length)\n",
        "    sequence2 = get_sequences(tokenizer, [preprocessed2], max_length)\n",
        "    if sentence1 == sentence2:\n",
        "      return 1\n",
        "    return model.predict([sequence1, sequence2])[0][0]"
      ],
      "metadata": {
        "id": "7akPkLB5txt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "new_sentence1 = \"its raining\"\n",
        "new_sentence2 = \"the weather is cloudy\"\n",
        "predicted_similarity = predict_similarity(model, new_sentence1, new_sentence2, tokenizer, max_length)\n",
        "print(f\"Predicted similarity score: {predicted_similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBIi26SluCFt",
        "outputId": "02404c18-f3de-4105-ab7a-ce2dd5afb0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 48ms/step\n",
            "Predicted similarity score: 0.5845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UUNB9PgSuD8m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ1lPu94zBrAbVaDTSudCr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}